{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1eftyRhBH30qOzfh3bx3t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dg2P0rK4g235","executionInfo":{"status":"ok","timestamp":1750159743643,"user_tz":-330,"elapsed":36363,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}},"outputId":"f5799cb7-1e6b-4a3a-f8f2-6ea110bf11db"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/ml_scratch/sigmoid.png  sigmoid.png"],"metadata":{"id":"FmHwFB4QlCSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sigmoid Function\n","\n","The **Sigmoid function** is an S-shaped function that maps any real-valued number into the range \\( (0, 1) \\). It is often used as an activation function in machine learning, especially in logistic regression and neural networks.\n","\n","The formula is:\n","\n","$$\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","### Explanation\n","\n","- Input value: $x \\in \\mathbb{R}$\n","- Output: $\\sigma(x) \\in (0, 1)$\n","- The function \"squashes\" input values into a probability-like output\n","- It is smooth and differentiable, which is useful for optimization\n","\n","The function is symmetric around \\( x = 0 \\), where $( \\sigma(0) = 0.5 )$. For large positive inputs, the output approaches 1; for large negative inputs, it approaches 0.\n","\n","\n","![Sigmoid Function](sigmoid.png)\n","\n","\n","\n","## Binary Cross-Entropy Loss\n","\n","**Binary Cross-Entropy (BCE)**, also known as **log loss**, is a commonly used loss function for binary classification problems. It measures the difference between two probability distributions: the predicted probabilities and the actual labels.\n","\n","The formula for binary cross-entropy loss is:\n","\n","$$\n","BCE = -\\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n","$$\n","\n","\n","### Explanation:\n","\n","- True label: $y \\in \\{0, 1\\}$\n","- Predicted probability: $\\hat{y} \\in (0, 1)$\n","- The function penalizes confident but incorrect predictions more strongly\n","\n","\n","This function penalizes confident but incorrect predictions more heavily. It is well-suited for models that output probabilities (e.g., using a sigmoid activation).\n","\n","For a dataset with multiple samples, the average BCE over all examples is typically computed:\n","\n","$$\n","\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\n","$$"],"metadata":{"id":"svdHV9YcbwDb"}},{"cell_type":"markdown","source":["# Logistic Regression Explained (Simple and Clear)\n","\n","**Logistic Regression** is one of the most popular algorithms in **machine learning**, especially for **binary classification problems**—where the goal is to classify data into one of two categories, such as spam or not spam, pass or fail, yes or no.\n","\n","---\n","\n","## 🔍 What Is Logistic Regression?\n","\n","Despite the name, **logistic regression is used for classification**, not regression. It predicts the **probability** that a given input belongs to a particular class. For example, it might predict whether an email is spam (`1`) or not spam (`0`).\n","\n","It works by applying a **logistic function (also called the sigmoid function)** to a linear combination of input features. This function squashes the output to a value between **0 and 1**, which can be interpreted as a probability.\n","\n","---\n","\n","## 🧮 The Math (Simplified)\n","\n","Let’s break it down step by step.\n","\n","### 1. Linear combination of features:\n","\n","\\[\n","z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b\n","\\]\n","\n","Where:\n","- \\( x_1, x_2, \\ldots, x_n \\) are the input features.\n","- \\( w_1, w_2, \\ldots, w_n \\) are the weights.\n","- \\( b \\) is the bias term.\n","\n","### 2. Apply the sigmoid function:\n","\n","$$\n","\\sigma(x) = \\frac{1}{1 + e^{-x}}\n","$$\n","\n","This transforms the output into a **probability** between 0 and 1.\n","\n","### 3. Make a prediction:\n","\n","- If the output probability is > 0.5 → predict **class 1**\n","- If ≤ 0.5 → predict **class 0**\n","\n","---\n","\n","## ✅ Why Use Logistic Regression?\n","\n","- Simple to **implement and interpret**\n","- Good for **linearly separable data**\n","- Outputs **probabilities**, not just labels\n","\n","---\n","\n","## 🧠 Training the Model\n","\n","Logistic regression is trained using **maximum likelihood estimation**. The model learns weights and bias that **maximize the likelihood** of the observed data.\n","\n","The loss function used is called **log loss** or **binary cross-entropy**, which penalizes incorrect and overconfident predictions.\n","\n","---\n","\n","## ⚠️ Limitations\n","\n","- Assumes a **linear relationship** between features and output (in log-odds space)\n","- Not suitable for **complex, non-linear patterns** without transformations\n","\n","---\n","\n","## 📦 Real-World Use Cases\n","\n","- Email **spam detection**\n","- **Loan default** prediction\n","- **Disease diagnosis** (e.g., predicting diabetes)\n","- **Customer churn** prediction\n","\n","---\n","\n","## 🧾 Conclusion\n","\n","Logistic Regression is a powerful and beginner-friendly **classification algorithm**. While it may not capture deep patterns in complex data, its **simplicity, speed, and interpretability** make it an excellent first choice for many binary classification problems.\n"],"metadata":{"id":"rMIRu1XpqlOM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"anRYM8SJbpbu"},"outputs":[],"source":["def compute_loss(y_true,y_pred):\n","  epsilon = 1e-9\n","  A=y_true*np.log(y_pred+epsilon)\n","  B=(1-y_true)*np.log(1-y_pred+epsilon)\n","  bce=-[A+B]\n","  return bce\n","\n","# def"]},{"cell_type":"code","source":["class LogisticRegression:\n","  def __init__(self,learning_rate=0.001, n_iters=1000):\n","    self.weights=None\n","    self.bias=None\n","    self.lr=learning_rate\n","    self.n_iters=n_iters\n","    self.losses=[]\n","\n","  def sigmoid(self,x):\n","    return 1/(1+np.exp(-x))\n","\n","  def compute_loss(self,y_true,y_pred):\n","    epsilon=1e-9\n","    y1=y_true*np.log(y_pred+epsilon)\n","    y2=(1-y_true)*np.log(1-y_pred+epsilon)\n","    return -np.mean(y1+y2)\n","\n","  def feed_forward(self,X):\n","    Z=np.dot(X,self.weights)+self.bias\n","    A=self.sigmoid(Z)\n","    return A\n","\n","  def fit(self,X,y):\n","    n_samples,n_features=X.shape\n","\n","    ##init parameters\n","    self.weights=np.zeros(n_features)\n","    self.bias=0\n","\n","    for _ in range(self.n_iters):\n","      A= self.feed_forward(X)\n","      self.losses.append(self.compute_loss(y,A))\n","      dz=A-y ## derivative of sigmoid\n","\n","      dw=(1/n_samples)*np.dot(X.T,dz)\n","      db=(1/n_samples)*np.sum(dz)\n","\n","      self.weights-=self.lr*dw\n","      self.bias-=self.lr*db\n","\n","  def predict(self,X):\n","    threshold = .5\n","    y_hat=np.dot(X,self.weights)+self.bias\n","    y_predicted=self.sigmoid(y_hat)\n","    y_predicted_cls = [1 if i > threshold else 0 for i in y_predicted]\n","\n","    return np.array(y_predicted_cls)"],"metadata":{"id":"tpYd_IC0rMAp","executionInfo":{"status":"ok","timestamp":1750162482991,"user_tz":-330,"elapsed":42,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","# from sklearn.linear_model import LogisticRegression\n","from sklearn import datasets\n","import numpy as np\n","\n","dataset = datasets.load_breast_cancer()\n","X, y = dataset.data, dataset.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=1234\n",")\n","\n","regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)  # scikit-learn's LogisticRegression, not your custom one\n","regressor.fit(X_train, y_train)\n","predictions = regressor.predict(X_test)\n","\n","cm = confusion_matrix(y_test, predictions)\n","accuracy = accuracy_score(y_test, predictions)\n","sens = recall_score(y_test, predictions)  # Sensitivity = Recall\n","precision = precision_score(y_test, predictions)\n","f_score = f1_score(y_test, predictions)\n","\n","print(\"Test accuracy: {0:.3f}\".format(accuracy))\n","print(\"Confusion Matrix:\\n\", cm)\n","print(\"Sensitivity (Recall): {0:.3f}\".format(sens))\n","print(\"Precision: {0:.3f}\".format(precision))\n","print(\"F1 Score: {0:.3f}\".format(f_score))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IE2FVYegzx2M","executionInfo":{"status":"ok","timestamp":1750162684566,"user_tz":-330,"elapsed":199,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}},"outputId":"9060d6e4-c531-4102-eb6f-2828fdbfbbef"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.930\n","Confusion Matrix:\n"," [[39  6]\n"," [ 2 67]]\n","Sensitivity (Recall): 0.971\n","Precision: 0.918\n","F1 Score: 0.944\n"]}]}]}