{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEFoZtrl+UZaBZUoHkIBXY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üß† SVM From Scratch with Gradient Descent\n","\n","This guide walks through the implementation of a **linear Support Vector Machine (SVM)** from scratch in Python using only `numpy`. The model uses **hinge loss** with **L2 regularization** and optimizes the objective using **stochastic gradient descent (SGD)**.\n","\n","---\n","\n","## üìå Objective Function\n","\n","The primal form of the SVM optimization problem is:\n","\n","$$\n","\\min_{\\mathbf{w}, b} \\ \\lambda \\cdot \\frac{1}{2} \\|\\mathbf{w}\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right)\n","$$\n","\n","- $\\mathbf{w}$ ‚Äî weight vector  \n","- $b$ ‚Äî bias  \n","- $\\lambda$ ‚Äî regularization parameter  \n","- $y_i \\in \\{-1, +1\\}$ ‚Äî class labels  \n","- $\\mathbf{x}_i \\in \\mathbb{R}^d$ ‚Äî feature vector\n","\n","---\n","\n","## üìê Gradient Derivation\n","\n","### Regularization Term\n","\n","$$\n","\\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\lambda \\cdot \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\right) = \\lambda \\mathbf{w}\n","$$\n","\n","$$\n","\\frac{\\partial}{\\partial b} \\left( \\lambda \\cdot \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\right) = 0\n","$$\n","\n","---\n","\n","### Hinge Loss Gradient\n","\n","If constraint is satisfied: $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1$\n","\n","$$\n","\\frac{\\partial L}{\\partial \\mathbf{w}} = \\lambda \\mathbf{w}, \\quad\n","\\frac{\\partial L}{\\partial b} = 0\n","$$\n","\n","If constraint is violated: $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) < 1$\n","\n","$$\n","\\frac{\\partial L}{\\partial \\mathbf{w}} = \\lambda \\mathbf{w} - y_i \\mathbf{x}_i, \\quad\n","\\frac{\\partial L}{\\partial b} = -y_i\n","$$\n","\n","---\n","\n","## ‚úÖ Summary\n","\n","- Uses binary labels $y \\in \\{-1, 1\\}$\n","- Hinge loss only penalizes if prediction is wrong or within margin\n","- Regularization keeps weights small and prevents overfitting\n"],"metadata":{"id":"OgY5DyLjPo5H"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_blobs\n","import numpy as np"],"metadata":{"id":"S2iRnpVyYe43","executionInfo":{"status":"ok","timestamp":1750690095343,"user_tz":-330,"elapsed":4258,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gGxiszeLYSZ3","executionInfo":{"status":"ok","timestamp":1750690095345,"user_tz":-330,"elapsed":5,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}}},"outputs":[],"source":["X, y = make_blobs(\n","    n_samples=250, n_features=2, centers=2, cluster_std=1.05, random_state=1\n",")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=1)\n"]},{"cell_type":"code","source":["class SVM:\n","  def __init__(self,learning_rate=1e-3, lambda_param=1e-2, n_iters=1000):\n","    self.learning_rate=learning_rate\n","    self.lambda_param=lambda_param\n","    self.n_iters=n_iters\n","    self.w=None\n","    self.b=None\n","\n","  def _init_weights_bias(self, X):\n","    n_features=X.shape[1]\n","    self.w=np.zeros(n_features)\n","    self.b=0\n","\n","  def _get_cls_map(self, y):\n","    return np.where(y<=0,-1,1)\n","\n","  def _satisfy_constraint(self,x,idx):\n","      linear_model=np.dot(x,self.w)+self.b\n","      return self.cls_map[idx] * linear_model >= 1\n","\n","  def _get_gradients(self,constraint,x,idx):\n","    if constraint:\n","      dw=self.lambda_param*self.w\n","      db=0\n","      return dw,db\n","\n","    dw=self.lambda_param*self.w-np.dot(self.cls_map[idx],x)\n","    db=-self.cls_map[idx]\n","    return dw,db\n","\n","  def _update_weights_bias(self,dw,db):\n","    self.w-=self.learning_rate*dw\n","    self.b-=self.learning_rate*db\n","\n","\n","  def fit(self, X, y):\n","        self._init_weights_bias(X)\n","        self.cls_map = self._get_cls_map(y)\n","\n","        for _ in range(self.n_iters):\n","            for idx, x in enumerate(X):\n","                constrain = self._satisfy_constraint(x, idx)\n","                dw, db = self._get_gradients(constrain, x, idx)\n","                self._update_weights_bias(dw, db)\n","\n","  def predict(self, X):\n","      estimate = np.dot(X, self.w) + self.b\n","      prediction = np.sign(estimate)\n","      return np.where(prediction == -1, 0, 1)\n"],"metadata":{"id":"zaHtQa3a3h5X","executionInfo":{"status":"ok","timestamp":1750690095378,"user_tz":-330,"elapsed":27,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["X, y = make_blobs(\n","    n_samples=250, n_features=2, centers=2, cluster_std=1.05, random_state=1\n",")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=1)\n","\n","clf = SVM(n_iters=1000)\n","clf.fit(X_train, y_train)\n","predictions = clf.predict(X_test)\n","\n","def accuracy(y_true, y_pred):\n","    accuracy = np.sum(y_true==y_pred) / len(y_true)\n","    return accuracy\n","\n","print(\"SVM Accuracy: \", accuracy(y_test, predictions))"],"metadata":{"id":"a7Dg1O3s_VCh","executionInfo":{"status":"ok","timestamp":1750690100473,"user_tz":-330,"elapsed":5088,"user":{"displayName":"mayur ml","userId":"12206475101738930241"}},"outputId":"413d64f0-cc19-4283-9360-8c9988e63989","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy:  1.0\n"]}]}]}